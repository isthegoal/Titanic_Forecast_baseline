{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn  #这是一个可视化的展示库\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'G:\\\\Machine-learning\\\\tensorflow-train-project(jupyter)\\\\kaggele-project6-tairanprediction\\\\dataset\\\\train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-54397fac77b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#直接使用pandas对数据进行直观展示观看\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'G:\\Machine-learning\\\\tensorflow-train-project(jupyter)\\kaggele-project6-tairanprediction\\dataset\\\\train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'G:\\\\Machine-learning\\\\tensorflow-train-project(jupyter)\\\\kaggele-project6-tairanprediction\\\\dataset\\\\train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#直接使用pandas对数据进行直观展示观看\n",
    "data=pd.read_csv('G:\\Machine-learning\\\\tensorflow-train-project(jupyter)\\kaggele-project6-tairanprediction\\dataset\\\\train.csv')\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前置函数部分\n",
    "这里会先准备好补全信息函数(对缺失值的考虑十分重要)，属性值转换函数，数据归一化函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#年龄信息补全函数：  这里 使用的方式是RandomForesetClassifier来填补\n",
    "def set_missing_ages(data):\n",
    "    age_df=data[['Age','Fare','Parch','SibSp','Pclass']]  #pandas的利用，抽取一部分特征作为年龄相关特征时使用  这里使用五个特征[年龄、船票价格、父母子女数、经济状况]\n",
    "    #print('age_df:',age_df)\n",
    "    known_age=age_df[age_df.Age.notnull()].as_matrix()#   这是一种抽离，把知道年龄的数据抽取出来作为一个 矩阵，这个矩阵有包括上面的5个特征值\n",
    "    #print('known_age:',known_age)\n",
    "    unknown_age=age_df[age_df.Age.isnull()].as_matrix()  \n",
    "    \n",
    "    #接下来使用的事RandomForestClassifier算法来补全年龄特征，这里补全特征的思路是  使用除年龄外的其他特征来作为补全年龄的依据\n",
    "    y=known_age[:,0]\n",
    "    x=known_age[:,1:]\n",
    "    #创建补全算法，并进行数据喂养  随机森林中参数意义为：n_estimators 子模型个数  random_state指定随机器对象 参数可参考https://www.cnblogs.com/amberdata/p/7203632.html  \n",
    "    rfr=RandomForestRegressor(random_state=0,n_estimators=2000,n_jobs=-1)\n",
    "    rfr.fit(x,y)\n",
    "    #使用训练好的随机森林进行预测\n",
    "    predictedAges=rfr.predict(unknown_age[:,1::])#对不知道年龄的矩阵，使用其特征进行年龄的预测\n",
    "    data.loc[(data.Age.isnull()),'Age']=predictedAges#这是针对DateFrame的定位并填充残缺值的方式\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将属性转换为数值的函数\n",
    "def attribute_to_number(data):\n",
    "    #前三行会对属性进行独热编码\n",
    "    dummies_Pclass=pd.get_dummies(data['Pclass'],prefix='Pclass')#get_dummies会对属性进行独热编码\n",
    "    dummies_Embarked=pd.get_dummies(data['Embarked'],prefix='Embarked')\n",
    "    dummies_Sex=pd.get_dummies(data['Sex'],prefix='Sex')\n",
    "    #这里会将独热编码结果放到  组合结果的后面\n",
    "    data=pd.concat([data,dummies_Pclass,dummies_Embarked,dummies_Sex],axis=1) #concat不管列名,直接加到一起,可以加到后面、也可以加到右边,axis=0为加到后面,axis=1为加到右边\n",
    "    #针对这个类型，将前面没用的非数值属性去掉(使用后面新加入的 转化好的属性即可)\n",
    "    data.drop(['Pclass','Sex','Embarked'],axis=1,inplace=True)\n",
    "    return data\n",
    "#对属性值进行归一化操作\n",
    "def Scales(data):\n",
    "    scaler=preprocessing.StandardScaler()#StandardScaler用于数据标准化，计算训练集的平均值和标准差,以便测试数据集使用相同的变换\n",
    "    #对年龄进行归一化处理\n",
    "    age_scale_param=scaler.fit(data['Age'].reshape(-1,1))#这样转换，使其具有0均值，单位方差\n",
    "    data['Age_scaled']=scaler.fit_transform(data['Age'].reshape(-1,1),age_scale_param)#fit_transform可以进行最大最小的标准化\n",
    "    #对工资进行归一化处理\n",
    "    Fare_scale_param=scaler.fit(data['Fare'].reshape(-1,1))\n",
    "    data['Fare_scaled']=scaler.fit_transform(data['Fare'].reshape(-1,1),Fare_scale_param)\n",
    "    #对兄弟配偶数进行归一化处理\n",
    "    SibSp_scale_param=scaler.fit(data['SibSp'].reshape(-1,1))\n",
    "    data['SibSp_scaled']=scaler.fit_transform(data['SibSp'].reshape(-1,1),SibSp_scale_param)\n",
    "    #对父母子女数进行归一化处理\n",
    "    Parch_scale_param=scaler.fit(data['Parch'].reshape(-1,1))\n",
    "    data['Parch_scaled']=scaler.fit_transform(data['Parch'].reshape(-1,1),SibSp_scale_param)\n",
    "    #使用归一化后的数据，将之前的数据删去\n",
    "    data.drop(['Parch','SibSp','Fare','Age'],axis=1,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分\n",
    "读取数据集之后，现在我们进行数据的预处理，预处理包括提取主要特征（这里经过我们思考，感觉年龄和性别以及座位等级也许是比较中的特征信息，因此我们会将多余的信息删去）、补全数据、数据的归一化这些思路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreProcess(in_data,submat_flg):#数据预处理    submat_flg为1表示是针对测试数据的处理\n",
    "    #核心的几个预处理操作，有些函数需要自己手动去写\n",
    "    in_data.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True) #删去无关特征\n",
    "    data_ages_fitted=set_missing_ages(in_data)#补足年龄信息\n",
    "    data=attribute_to_number(data_ages_fitted)#类目属性转为数值型\n",
    "    data_scaled=Scales(data)#将转化为数值型的特征数据进行归一化\n",
    "    \n",
    "    #对特征数据和标签数据的划分   进行划分后作为新的数据返回\n",
    "    data_copy=data_scaled.copy(deep=True)  #将数据集进行完全的备份   深拷贝\n",
    "    data_copy.drop(['Pclass_1','Pclass_2','Pclass_3','Embarked_C','Embarked_Q','Embarked_S','Sex_female','Sex_male','Age_scaled','Fare_scaled','SibSp_scaled','Parch_scaled'],axis=1,inplace=True)\n",
    "    data_y=np.array(data_copy)#这里是删去了很多东西(进行一种选择性的删除，还有axis这个自己不太掌握)   剩下的作为y的标签\n",
    "    if submat_flg==0:\n",
    "        data_scaled.drop(['Survived'],axis=1,inplace=True)\n",
    "        data_X=np.array(data_scaled)#特征组合x，这是要进行网络输入所用的所有特征值\n",
    "        return data_X,data_y\n",
    "    if(submat_flg==1):\n",
    "        data_X=np.array(data_scaled)#特征组合x，这是要进行网络输入所用的所有特征值\n",
    "        return data_X,data_y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分\n",
    "基于之前的逻辑回归进行修改，在模型部分更改即可\n",
    "基本思路：创建变量    用变量去执行网络，主要的区别是之前是使用模型计算得出结果，现在是获取得到网络的softmax输出值。。。   \n",
    "这里打算创建一个五层的神经网络，三隐层即可  (原型中包装的有问题，全写在一起了，这样不好，因为现在我想拿模型进行预测时发现，擦，我还得再带入模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(data_X,data_y):\n",
    "    training_epochs=3\n",
    "    learning_rate=0.01\n",
    "    display_step=2\n",
    "    batch_size=10\n",
    "    logs_train_dir=\"G:\\Machine-learning\\\\tensorflow-train-project(jupyter)\\kaggele-project6-tairanprediction\\logs\\\\\"\n",
    "    #同样进行  喂养集和校验集的分离\n",
    "    X_train,X_test,y_train,y_test=train_test_split(data_X,data_y,test_size=0.2,random_state=0)\n",
    "    y_train=tf.concat([1-y_train,y_train],1)  #concat用于连接两个矩阵   1是指连接的维度（从哪个角度）\n",
    "    y_test=tf.concat([1-y_test,y_test],1)\n",
    "    #创建网络中使用的主要的变量\n",
    "    n_samples=X_train.shape[0]#得到训练集数量\n",
    "    n_feature=X_train.shape[1]#进行训练的特征数\n",
    "    n_class=2#标签的数值代表分类  是2分类\n",
    "    x=tf.placeholder(tf.float32,[None,n_feature]) #传入float类型的参数即可\n",
    "    y=tf.placeholder(tf.float32,[None,n_class])\n",
    "    #做好准备工作， 现在开始正式搭建神经网络的过程\n",
    "    with tf.name_scope(\"layer_in\"): #第一层的网络设置\n",
    "        Weight1=tf.Variable(tf.truncated_normal([n_feature,256],stddev=0.1),name='W1')\n",
    "        bias1=tf.Variable(tf.truncated_normal([256],stddev=0.1),name='b1')\n",
    "        hidden1=tf.nn.relu(tf.matmul(x,Weight1)+bias1,name='hidden1')\n",
    "    with tf.name_scope(\"layer_hidden_1\"):\n",
    "        Weight2=tf.Variable(tf.truncated_normal([256,128],stddev=0.1),name='W2')\n",
    "        bias2=tf.Variable(tf.truncated_normal([128],stddev=0.1),name='b2')\n",
    "        hidden2=tf.nn.relu(tf.matmul(hidden1,Weight2)+bias2,name='hidden2')\n",
    "    with tf.name_scope('layer_hidden_2'):\n",
    "        Weight3=tf.Variable(tf.truncated_normal([128,64],stddev=0.1),name='W3')\n",
    "        bias3=tf.Variable(tf.truncated_normal([64],stddev=0.1),name='b3')\n",
    "        hidden3=tf.nn.relu(tf.matmul(hidden2,Weight3)+bias3,name='hidden3')\n",
    "    with tf.name_scope('layer_out'):\n",
    "        Weight4=tf.Variable(tf.truncated_normal([64,2],stddev=0.1),name='W4')\n",
    "        bias4=tf.Variable(tf.truncated_normal([2],stddev=0.1),name='b4')\n",
    "        prediction=tf.nn.softmax(tf.matmul(hidden3,Weight4)+bias4,name='y_out')  #二分类获取输出即可\n",
    "    \n",
    "    #这里再计算损失值时  为了防止网络太复杂导致过拟合问题，所以我们这里使用正则化方法对求损失值函数进行限制（进行L2正则化）\n",
    "    vars=tf.trainable_variables()#可以获取所有需要训练的变量列表，这样才能整体对参数进行调整\n",
    "    lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if 'b' not in v.name]) * 0.05#设置L2正则化 的式子\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y)+lossL2) \n",
    "    \n",
    "    train_step=tf.train.MomentumOptimizer(learning_rate,0.9).minimize(cost)\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.arg_max(prediction,1),tf.arg_max(y,1))#arg_max用于返回vector中 最大值的索引号\n",
    "    accurary=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    ckpt_dir='./logs'\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    saver=tf.train.Saver()\n",
    "    init=tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:       \n",
    "        tf.global_variables_initializer().run()\n",
    "        ckpt=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        if ckpt:\n",
    "            print('Restoring from checkpoint: %s' % ckpt,'        path:',ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess,ckpt)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "            print('epoch:',epoch)\n",
    "            avg_cost=0\n",
    "            total_batch=int(n_samples/batch_size)   #在每个训练批次里面  我们都会使用所有的数据进行训练，所以这里会有个除式计算\n",
    "            for i in range(total_batch):\n",
    "                _,c=sess.run([train_step,cost],feed_dict={x:X_train[i*batch_size:(i+1)*batch_size],y:y_train[i*batch_size:(i+1)*batch_size,:].eval()})\n",
    "                avg_cost=c/total_batch#单批次下的损失值\n",
    "            plt.plot(epoch+1,avg_cost,'co')#把损失值变化 用plt图标来展示出来\n",
    "            \n",
    "            if(epoch+1)%display_step==0:#达到一定批次之后进行准确的输出展示\n",
    "                print('Epoch:',(epoch+1),',cost=',avg_cost)\n",
    "                #每个5步数，并进行模型结构和参数的保存\n",
    "                checkpoint_path=os.path.join(logs_train_dir,'model.ckpt')\n",
    "                saver.save(sess, ckpt_dir + '/logistic.ckpt')\n",
    "               # print('the Weight: ',sess.run(W),'  ,the bias:',sess.run(b))\n",
    "        X_test=sess.run(tf.convert_to_tensor(X_test))\n",
    "        y_test=sess.run(tf.convert_to_tensor(y_test))\n",
    "        print('Testing Accuracy:',sess.run(accurary,feed_dict={x:X_test,y:y_test}))#这个地方如果这样用，往feed_dict中传的是张量，这是tf的理解难点\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('cost')\n",
    "        plt.show()\n",
    "    sess.close()\n",
    "        #W=sess.run(W)\n",
    "        #b=sess.run(b)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x 的格式为: (891, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submat_test_data_x 的格式为: (418, 12)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'model_checkpoint_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-07d686bdc8f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msubmat_test_data_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubmat_test_data_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataPreProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmat_test_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submat_test_data_x 的格式为:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubmat_test_data_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m#print('the W:',W,',it type is',type(W))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#print('the b:',b,',it type is',type(b))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-6feba5a369ce>\u001b[0m in \u001b[0;36mNN\u001b[1;34m(data_X, data_y)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mckpt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Restoring from checkpoint: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'        path:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'model_checkpoint_path'"
     ]
    }
   ],
   "source": [
    "#调用函数来启动计算过程  \n",
    "if __name__==\"__main__\":\n",
    "    data=pd.read_csv('G:\\Machine-learning\\\\tensorflow-train-project(jupyter)\\kaggele-project6-tairanprediction\\dataset\\\\train.csv')#用padas读取csv文件\n",
    "    submat_test_data=pd.read_csv('G:\\Machine-learning\\\\tensorflow-train-project(jupyter)\\kaggele-project6-tairanprediction\\dataset\\\\test.csv')\n",
    "    submat_test_data_PassengerId=submat_test_data['PassengerId']\n",
    "    data_x,data_y=DataPreProcess(data,0)\n",
    "    print('data_x 的格式为:',data_x.shape)\n",
    "    submat_test_data_x,submat_test_data_y=DataPreProcess(submat_test_data,1)\n",
    "    print('submat_test_data_x 的格式为:',submat_test_data_x.shape)\n",
    "    NN(data_x,data_y)\n",
    "    #print('the W:',W,',it type is',type(W))\n",
    "    #print('the b:',b,',it type is',type(b))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 终篇\n",
    "使用训练出的网络对结果进行预测，读取网络模型，并将网络运行的结果保存在csv文件中去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submat_test_data_x  的格式: (418, 12)\n",
      "n_feature: 12\n",
      "测试  ！！！！\n",
      "测试  ！\n",
      "测试 \n",
      "测试 1\n",
      "测试 6\n",
      "测试 7\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 256 and 128 for 'layer_hidden_2_9/MatMul' (op: 'MatMul') with input shapes: [?,256], [128,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    687\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 256 and 128 for 'layer_hidden_2_9/MatMul' (op: 'MatMul') with input shapes: [?,256], [128,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3ccd797f5878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubmat_test_data_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mlast_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-3ccd797f5878>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mb3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'测试 7'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mhidden3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hidden3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer_out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mW4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'W4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[1;32m-> 1891\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   2434\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   2435\u001b[0m         \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2436\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   2437\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2438\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2956\u001b[0m         op_def=op_def)\n\u001b[0;32m   2957\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2958\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2209\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2210\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 256 and 128 for 'layer_hidden_2_9/MatMul' (op: 'MatMul') with input shapes: [?,256], [128,64]."
     ]
    }
   ],
   "source": [
    "#自己包装的不好，得再建个函数     输入x，放回最后的预测结果\n",
    "def test_model(X):\n",
    "    n_feature=X.shape[1]\n",
    "    print('n_feature:',n_feature)\n",
    "    #x=tf.placeholder(tf.float32,[None,n_feature]) #传入float类型的参数即可\n",
    "    #做好准备工作， 现在开始正式搭建神经网络的过程\n",
    "    with tf.name_scope(\"layer_in\"): #第一层的网络设置\n",
    "        print('测试  ！！！！')\n",
    "        W1=tf.Variable(tf.truncated_normal([12,256],stddev=0.1),name='W1')\n",
    "        print('测试  ！')\n",
    "        b1=tf.Variable(tf.truncated_normal([256],stddev=0.1),name='b1')\n",
    "        print('测试 ')\n",
    "        hidden1=tf.nn.relu(tf.matmul(X,W1)+b1,name='hidden1')\n",
    "        print('测试 1')\n",
    "    with tf.name_scope(\"layer_hidden_1\"):\n",
    "        print('测试2 ')\n",
    "        W2=tf.Variable(tf.truncated_normal([256,128],stddev=0.1),name='W2')\n",
    "        print('测试3 ')\n",
    "        b2=tf.Variable(tf.truncated_normal([128],stddev=0.1),name='b2')\n",
    "        print('测试 4')\n",
    "        hidden2=tf.nn.relu(tf.matmul(hidden1,W2)+b2,name='hidden2')\n",
    "        print('测试5 ')\n",
    "    with tf.name_scope('layer_hidden_2'):\n",
    "        W3=tf.Variable(tf.truncated_normal([128,64],stddev=0.1),name='W3')\n",
    "        print('测试 6')\n",
    "        b3=tf.Variable(tf.truncated_normal([64],stddev=0.1),name='b3')\n",
    "        print('测试 7')\n",
    "        hidden3=tf.nn.relu(tf.matmul(hidden2,W3)+b3,name='hidden3')\n",
    "    with tf.name_scope('layer_out'):\n",
    "        W4=tf.Variable(tf.truncated_normal([64,2],stddev=0.1),name='W4')\n",
    "        print('测试 9')\n",
    "        b4=tf.Variable(tf.truncated_normal([2],stddev=0.1),name='b4')\n",
    "        print('测试 10',b4)\n",
    "        \n",
    "        prediction=tf.nn.softmax(tf.matmul(hidden3,W4)+b4,name='y_out')  #二分类获取输出即可\n",
    "        last_result=tf.arg_max(prediction,1)\n",
    "        print('测试 11')\n",
    "    return last_result\n",
    "\n",
    "#读取保存的网络模型\n",
    "logs_train_dir=\"G:\\Machine-learning\\\\tensorflow-train-project(jupyter)\\kaggele-project6-tairanprediction\\logs\\\\\"\n",
    "saver=tf.train.Saver()\n",
    "print('submat_test_data_x  的格式:',submat_test_data_x.shape)\n",
    "n_features=submat_test_data_x.shape[1]\n",
    "X=tf.placeholder(tf.float32,[None,n_features])\n",
    "last_result=test_model(X)\n",
    "init=tf.initialize_all_variables()\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(init)\n",
    "    #读取保存的ckpt文件\n",
    "    \n",
    "    ckpt = tf.train.latest_checkpoint(logs_train_dir)\n",
    "    if ckpt:\n",
    "        print('Restoring from checkpoint: %s' % ckpt)\n",
    "        saver.restore(sess2, ckpt)\n",
    "\n",
    "    #ckpt=tf.train.get_checkpoint_state(logs_train_dir)\n",
    "    #if ckpt and ckpt.model_checkpoint_path:\n",
    "       # global_step=ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "       # saver.restore(sess2,ckpt.model_checkpoint_path)\n",
    "    #现在已经加载了之前保存的会话了。      \n",
    "    predictions_to=sess2.run([last_result],feed_dict={X:submat_test_data_x})\n",
    "    print('predictions:',predictions_to)  \n",
    "    #print('predictions:',predictions_to.shape)                                                                   \n",
    "submission=pd.DataFrame({'PassengerId':submat_test_data_PassengerId,'Survived':predictions_to[0]})#构造一个表，用于放置预测结果    predictions_to[0]语句相当于把这取出来了\n",
    "submission.to_csv('submit.csv',index=False)\n",
    "\n",
    "\n",
    "#使用计算得到的W和b参数重新启动计算过程     在会话中进行获救结果的预测，并保持在一个csv文件中\n",
    "# #创建计算图点\n",
    "# n_features=submat_test_data_x.shape[1]\n",
    "# X=tf.placeholder(tf.float32,[None,n_features])\n",
    "# yHat=tf.add(tf.matmul(X,W),B)\n",
    "# last_result=tf.arg_max(yHat,1)   #构建计算图，直接放置即可    自己想出来的方法\n",
    "# with tf.Session() as sess2:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     submat_test_data_x=sess2.run(tf.convert_to_tensor(submat_test_data_x))\n",
    "#     Weight=sess2.run(tf.convert_to_tensor(Weight))\n",
    "#     bias=sess2.run(tf.convert_to_tensor(bias))\n",
    "#     print('submat shape:',submat_test_data_x.shape)\n",
    "#     print('Weight shape:',Weight.shape)\n",
    "#     print('bias shape:',type(bias))\n",
    "#     predictions_to=sess2.run([last_result],feed_dict={X:submat_test_data_x})#np.argmax用于得到数组最大值的索引  整体获得预测结果\n",
    "#     #predictions_to=sess2.run(tf.arg_max(tf.convert_to_tensor(predictions),1))\n",
    "#     #print('predictions:',predictions_to[0].shape)\n",
    "# submission=pd.DataFrame({'PassengerId':submat_test_data_PassengerId,'Survived':predictions_to[0]})#构造一个表，用于放置预测结果    predictions_to[0]语句相当于把这取出来了\n",
    "# submission.to_csv('submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for G:/logistic",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-c2148d2913b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/logistic\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mvar_to_shape_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_to_shape_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_to_shape_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m    156\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for G:/logistic"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_dir = 'G:/Machine-learning/tensorflow-train-project(jupyter)/kaggele-project6-tairanprediction/logs/'\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "#checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "checkpoint_path = os.path.join(model_dir, \"/logistic\")\n",
    "reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "for key in var_to_shape_map:\n",
    "    print(\"tensor_name: \", key)\n",
    "    print(reader.get_tensor(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
